%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Tufte Essay
% LaTeX Template
% Version 2.0 (19/1/19)
%
% This template originates from:
% http://www.LaTeXTemplates.com
%
% Authors:
% The Tufte-LaTeX Developers (https://www.ctan.org/pkg/tufte-latex)
% Vel (vel@LaTeXTemplates.com)
%
% License:
% Apache License, version 2.0
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[a4paper]{tufte-handout} % Use A4 paper by default, remove 'a4paper' for US letter

\usepackage{graphicx} % Required for including images
\setkeys{Gin}{width=\linewidth, totalheight=\textheight, keepaspectratio} % Default images settings
\graphicspath{{Figures/}{./}} % Specifies where to look for included images (trailing slash required)

\usepackage{amsmath, amsfonts, amssymb, amsthm} % For math equations, theorems, symbols, etc
\usepackage{units} % Non-stacked fractions and better unit spacing

\usepackage{booktabs} % Required for better horizontal rules in tables

%----------------------------------------------------------------------------------------
%	TITLE SECTION
%----------------------------------------------------------------------------------------

\title{Notes on Big Data Computing}

\author{Filippo Mazzarotto}

\date{A.Y. 2024/2025}

%----------------------------------------------------------------------------------------

\begin{document}

\maketitle % Print the title section

\begin{abstract}
	\textbf{Abstract.}
\end{abstract}

The field of Big Data Computing has gained significant importance due to the exponential increase in data generation. This surge in data necessitates efficient processing techniques, as traditional algorithms fall short in handling the sheer volume and complexity.

Big Data is characterized by the 5 V's:
\begin{itemize}
	\item \textbf{Volume}: The vast amount of data generated every second.
	\item \textbf{Velocity}: The speed at which new data is generated and processed.
	\item \textbf{Variety}: The different types of data (structured, unstructured, semi-structured).
	\item \textbf{Veracity}: The uncertainty and trustworthiness of data.
	\item \textbf{Value}: The potential insights and benefits derived from data.
\end{itemize}

This course primarily focuses on the challenges associated with Volume and Velocity. 

In the realm of Big Data, there is a constant tradeoff between accuracy and resource consumption (time, space, and costs, especially in cloud environments). To manage this tradeoff, non-optimal algorithms are often employed to conserve resources.

Effective Big Data solutions require:
\begin{itemize}
	\item \textbf{Data Cleaning}: Ensuring data quality and consistency.
	\item \textbf{Automated Analytics}: Leveraging machine learning and AI to derive insights.
	\item \textbf{Efficient Processing}: Utilizing distributed computing frameworks like Hadoop and Spark.
\end{itemize}
The goal is to achieve easy management and programming to save time and reduce complexity in handling large datasets.

Map Reduce: theoretical model, not a real implementation. It's a way to think about how to process data.
Apache Spark: a real implementation of Map Reduce, with a lot of optimizations.

Both are based on distributed and parallel computing. The techniques to speed up processing include:
\begin{itemize}
	\item \textbf{Partitioning}: Dividing data into smaller, manageable chunks.
	\item \textbf{Sketches}: Estimation techniques for moments and set membership.
	\item \textbf{Locality Sensitive Hashing (LSH)}: Used for similarity search.
\end{itemize}

In distributed systems, reliability is a key concern. An important metrics is the \textbf{MTBF (Mean Time Between Failures)}, which is the average time between two failures.

Given $n$ nodes and the probability $p$ of a single node failing, the probability of at least one failure in the system is:
\[ p_n = 1 - (1-p)^n \]

Given $x$, the time passed before the first failure, the MTBF can be approximated as:
\[ MTBF = E[x] = \sum_{t=1}^{\infty} t \cdot (1-p)^{(t-1)n} \]
As $n$ grows, MTBF approaches \( \frac{1}{p} \).

\section{The Map Reduce Theoretical Framework}
Google introduced the MapReduce framework in 2003, developed by Jeffrey Dean and Sanjay Ghemawat \cite{ghemawat2003google}, \cite{dean2004mapreduce}. This programming framework is designed for processing large datasets on distributed platforms.

Dean and Ghemawat's 2008 paper provides a comprehensive overview of the programming model and its implementation \cite{dean2008mapreduce}. The framework is also inspired by concepts from functional programming.

MapReduce can be deployed on various cloud service models, including Infrastructure as a Service (IaaS), Platform as a Service (PaaS), and Software as a Service (SaaS).

In a distributed file system (DFS) like Hadoop's HDFS, files are divided into chunks of approximately 64MB and replicated across the cluster to ensure reliability and fault tolerance. The overall disk space refers to the combined disk capacity of all nodes in the cluster, while the main memory refers to the RAM available on a single node.

\section{Map Reduce implementations}

Hadoop / Spark. The latter is newer and faster. 

Input: key-value pairs
map: takes each (k,v) and returns a list 0..m of key-value pairs
shuffle: group by key
reduce: takes a key and its list of values and returns a list of key-value pairs
output: key-value pairs.

algorithms declare map and reduce functions. Shuffling is done automatically.
We want the number of reducers to be the similar to the number of processorcs.
The mappers usually prefer having access to some read-only global-shared values (such as numer of documents, etc.) to optimize.

Spark manages a lot. But it introduces a lot of overhead. Using it is part of the tradeoff between accuracy and resources.
Custom architecture is faster, but requires time, skill, money. For non-repetitive tasks, it's not worth it.

Anatomy of a Round

User program: master process and executor processes.
Master process assigns map/reduce tasks to executors and monitors their status (idle, running, completed).
Executor processes run map/reduce tasks and store intermediate results in their local memory.

\section{Map Reduce Analysis}

Parameters to consider:
1. Number of rounds
2. $M_L$, memory needed by a single invocation of map/reduce
3. $M_A$, memory needed to each executor


\newpage

\bibliography{sample.bib}
\bibliographystyle{plainnat}


\end{document}
