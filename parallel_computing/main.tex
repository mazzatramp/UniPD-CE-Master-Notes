%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Tufte Essay
% LaTeX Template
% Version 2.0 (19/1/19)
%
% This template originates from:
% http://www.LaTeXTemplates.com
%
% Authors:
% The Tufte-LaTeX Developers (https://www.ctan.org/pkg/tufte-latex)
% Vel (vel@LaTeXTemplates.com)
%
% License:
% Apache License, version 2.0
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[a4paper]{tufte-handout} % Use A4 paper by default, remove 'a4paper' for US letter

\usepackage{graphicx} % Required for including images
\setkeys{Gin}{width=\linewidth, totalheight=\textheight, keepaspectratio} % Default images settings
\graphicspath{{Figures/}{./}} % Specifies where to look for included images (trailing slash required)

\usepackage{amsmath, amsfonts, amssymb, amsthm} % For math equations, theorems, symbols, etc
\usepackage{units} % Non-stacked fractions and better unit spacing

\usepackage{booktabs} % Required for better horizontal rules in tables

%----------------------------------------------------------------------------------------
%	TITLE SECTION
%----------------------------------------------------------------------------------------

\title{Notes on Parallel Computing}

\author{Filippo Mazzarotto}

\date{A.Y. 2024/2025} % Date, use \date{} for no date

%----------------------------------------------------------------------------------------

\begin{document}

\maketitle % Print the title section

%----------------------------------------------------------------------------------------
%	ABSTRACT/SUMMARY
%----------------------------------------------------------------------------------------

\begin{abstract}
	\textbf{Abstract:} 
\end{abstract}

%----------------------------------------------------------------------------------------
%	GLOSSARY
%----------------------------------------------------------------------------------------

% To create a glossary in LaTeX, you need to use the 'glossaries' package.
% First, include the package in the preamble:
% \usepackage[acronym]{glossaries}

% Then, define your glossary entries. For example:
% \newglossaryentry{parallel_computing}{
%     name=parallel computing,
%     description={A type of computation in which many calculations or processes are carried out simultaneously}
% }

% For acronyms, you can use:
% \newacronym{gpu}{GPU}{Graphics Processing Unit}

% After defining your entries, you need to print the glossary in your document:
% \printglossaries

% Finally, to reference a glossary entry in your text, use:
% \gls{parallel_computing} or \gls{gpu}

%\section{Introduction}
%
%small histroy on parallel computers
%explicit and implicit parallelism
%Parallel computing is superset of non-parallel computing
%GHz can't go up, so we need to go wide
%
%top-500 \cite{top500}
%
%importance of network, as movement is the bottleneck
%importance of power consumption and how it went down
%
%Seymour Cray \cite{crey_seymour}

\section{Introduction to Parallelism}

Parallel computing is the term used to describe the simultaneous execution of multiple tasks. Sequential computing is basically a special case. The need of parallel computing appeard in the xxxx, when the clock speed of CPUs started to plateau. The only way to increase the computational power was to increase the number of cores, a challenge that was though to be impossible for many algorithms by many people.

Parallelism is quite a generic term, and can be achieved in many ways. We talk about explicit and implicit parallelism.

\begin{itemize}
	\item In \textbf{Explicit parallelism} the programmer specifies parallel tasks and their execution. Requires knowledge of hardware and parallel constructs (e.g., threads, message passing, synchronization). Examples: OpenMP, MPI, CUDA.
	\item In \textbf{Implicit parallelism} the compiler or the processor automatically parallelizes the code. The program itself has sequential semantics.
\end{itemize}

Parallelism is used by Supercomputers to achieve their high performance. The TOP500\cite{top500} lists the most powerful computer systems, ranking them by their performance on the LINPACK benchmark (a measure of a system's floating-point computing power). 

The primary bottleneck for large computing systems is the communication between processors, as data movement is limited by the speed of light. The power consumption of supercomputers is also a major concern, but the efficiency has been increasing by many orders of magnitude over the years.

Seymour Cray is the pioneer in supercomputing. He made substantial contributions to the field\cite{crey_seymour} setting the foundation for many of today's parallel computing architectures. 

\subsection{Supercomputer's tasks}

Parallel computing is and was used in many scientific fields. Including:

\textbf{Physics}
In physics, particularly in Quantum Electrodynamics (QED) and Quantum Chromodynamics (QCD), complex numbers are used extensively. While it is theoretically possible to compute these using boolean gates in-chip, such optimizations are not practical for general-purpose computing. String theory simulations require even more computational power.

\textbf{Astronomy}
Astronomy benefits from parallel computing through simulations of the universe. This is done using clusters of galaxies.

\textbf{Biology}
In biology, protein folding is the most significant challenge. IBM's 2002 project on protein folding\cite{ibm_protein_folding} and DeepMind's AlphaFold in 2020\cite{alphafold} have made substantial progress in this area using parallel computing.

\textbf{Neuroscience}
Neuroscience aims to simulate the human brain, which has approximately $10^{11}$ neurons and $10^{15}$ synapses, requiring around $10^{18}$ operations per second. While a full simulation of the human brain is not yet feasible, parallel computing is essential for progress in this field.

\textbf{Artificial Intelligence and Machine Learning}
Large Language Models (LLMs) in AI and ML have billions of parameters ($10^9$). Training these models is computationally expensive, requiring around $10^{19}$ operations, while inference is cheaper, requiring around $10^9$ operations.

\textbf{Cryptography}
Cryptography relies heavily on parallel computing for tasks such as RSA encryption\cite{rsa} and homomorphic cryptography\cite{homomorphic_cryptography}.

\section{Parallel Architectures}

We don't know what makes a problem difficult to compute.

The NC class of problems is defined as those that can be solved in logarithmic time using a polynomial number of processors. Problems in NC can be efficiently solved in parallel. However, if a problem is not in NC, it might still be beneficial to solve it in parallel under certain conditions.

The Circuit Value Problem is in NC. The problem is to determine the value of a circuit given the values of the inputs. Given a number of processors equal to the number of gates, we can compute the value of the circuit in logarithmic time.

\subsection{Example on Prallelizing Algorithms: the Prefix Sum}

Without parallelism, the prefix sum computation has a time complexity of $O(n)$. 

With parallelism, we can achieve better performance.

\textbf{Top-down approach:} Using divide and conquer, and the associative property of sum, we can divide the sum in half and sum the two halves in different processors. This is a recursive algorithm. The base case is when the sum is of size 2, in which case we can just return the sum. This approach leverages the power of parallel processors to perform the summation in parallel, reducing the overall time complexity.

\textbf{Bottom-up approach:} We can use a tree structure to sum the numbers. The tree is a binary tree, with the leaves being the numbers to sum. The internal nodes represent the sum of their children. The root node will eventually hold the sum of all the numbers. This approach is inherently parallel as each level of the tree can be processed simultaneously.

Both approaches lead to a time complexity of $O(\log n)$, and this is true for any associative operation. This significant reduction in time complexity demonstrates the power of parallel computing in optimizing algorithms.

\section{Communication}

Communication is the bottleneck in parallel computing. Speed of light is the limit. 

\section{Parallel Algorithms}

Parallelism on Merge sort.

\newpage

\bibliography{sample.bib} 
\bibliographystyle{plainnat}


\end{document}
